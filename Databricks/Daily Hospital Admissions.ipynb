{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4161717a-2938-44ea-bd24-b79025d154ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "hospital_admissions = spark.read.csv(\"/mnt/bronze/hospital_admissions.csv\", header=True, inferSchema = True)\n",
    "hospital_admissions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d33d4146-832e-470c-827a-dace05b982bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "columns_needed = ['country','indicator','date','year_week','value']\n",
    "hospital_admissions_df = hospital_admissions.select(columns_needed)\n",
    "hospital_admissions_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6eb8e27d-92dd-44ac-bd45-a5b21db663a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create a new week column with the last 2 characters of the year_week column\n",
    "hospital_admissions_df = hospital_admissions_df.withColumn(\"week\", col(\"year_week\").substr(7, 2))\n",
    "\n",
    "\n",
    "hospital_admissions_df.show()\n",
    "\n",
    "# Get rid of year_week column\n",
    "hospital_admissions_df  = hospital_admissions_df.drop('year_week')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e48e5962-a9c0-4d72-8430-06924b75c4c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "# Get the week start date\n",
    "week_start_df = hospital_admissions_df.groupBy(\"week\").agg(\n",
    "    F.min(\"date\").alias(\"week_start_date\")  # Get the earliest date for each week\n",
    ")\n",
    "\n",
    "# Show the resulting DataFrame with week start dates\n",
    "week_start_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80a60cc4-d6a8-4131-a208-6b6f37eed00e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Join to Dataframe to get the week start date\n",
    "hospital_admissions_df = hospital_admissions_df.join(\n",
    "    week_start_df, on=\"week\", how=\"left\"  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19fe85c8-2d77-4da0-b63b-64b4d3164b55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Pivot to have indicator values as columns\n",
    "hospital_admissions_df = hospital_admissions_df.groupBy(\"country\", \"date\", \"week\").pivot(\"indicator\").agg({\"value\": \"first\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d95798ee-14f4-4530-ae89-53fee7b6a810",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the names of the last two columns\n",
    "columns_to_drop = hospital_admissions_df.columns[-2:]\n",
    "\n",
    "# Create a new DataFrame without the last two columns\n",
    "hospital_admissions_df = hospital_admissions_df.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18f1c2b3-2948-4a10-aa3e-85b38fe8ff1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the transformed data into our silver layer\n",
    "output_path = \"abfss://ecdc@adlsmzubac125.dfs.core.windows.net/silver/dailyhospitaladmissions.csv\"\n",
    "\n",
    "# Save Dataframe as csv in data lake \n",
    "hospital_admissions_df.write.csv(output_path)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Daily Hospital Admissions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
