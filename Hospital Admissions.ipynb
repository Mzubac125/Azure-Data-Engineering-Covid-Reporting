{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b659bfcf-242a-4509-be83-1936f1b5c2dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/mnt/ecdc/cases_deaths.csv</td><td>cases_deaths.csv</td><td>14445098</td><td>1731360146000</td></tr><tr><td>dbfs:/mnt/ecdc/country_response.csv</td><td>country_response.csv</td><td>47308</td><td>1731360145000</td></tr><tr><td>dbfs:/mnt/ecdc/hospital_admissions.csv</td><td>hospital_admissions.csv</td><td>1058540</td><td>1731360146000</td></tr><tr><td>dbfs:/mnt/ecdc/testing.csv</td><td>testing.csv</td><td>85887</td><td>1731360145000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/mnt/ecdc/cases_deaths.csv",
         "cases_deaths.csv",
         14445098,
         1731360146000
        ],
        [
         "dbfs:/mnt/ecdc/country_response.csv",
         "country_response.csv",
         47308,
         1731360145000
        ],
        [
         "dbfs:/mnt/ecdc/hospital_admissions.csv",
         "hospital_admissions.csv",
         1058540,
         1731360146000
        ],
        [
         "dbfs:/mnt/ecdc/testing.csv",
         "testing.csv",
         85887,
         1731360145000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs\n",
    "ls \"mnt/bronze\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "839366b6-1c26-418e-9d79-566fd02f2a43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "hospital_admissions = spark.read.csv(\"/mnt/bronze/hospital_admissions.csv\", header=True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ae81842-d81d-4e85-8273-1810313c95bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----------+---------+------+---------------+--------------------+\n|country|           indicator|      date|year_week| value|         source|                 url|\n+-------+--------------------+----------+---------+------+---------------+--------------------+\n|Austria|Daily hospital oc...|2020-04-02| 2020-W14|1057.0|   Surveillance|https://www.sozia...|\n|Austria|Daily hospital oc...|2020-04-08| 2020-W15|1096.0|   Surveillance|https://www.sozia...|\n|Austria|Daily hospital oc...|2020-04-15| 2020-W16|1001.0|   Surveillance|https://info.gesu...|\n|Austria|Daily hospital oc...|2020-04-16| 2020-W16| 967.0|   Surveillance|https://www.sozia...|\n|Austria|Daily hospital oc...|2020-04-17| 2020-W16| 909.0|   Surveillance|https://www.sozia...|\n|Austria|Daily hospital oc...|2020-04-19| 2020-W16| 817.0|   Surveillance|https://www.sozia...|\n|Austria|Daily hospital oc...|2020-04-21| 2020-W17| 756.0|   Surveillance|https://www.sozia...|\n|Austria|Daily hospital oc...|2020-04-22| 2020-W17| 700.0|   Surveillance|https://www.sozia...|\n|Austria|Daily hospital oc...|2020-04-23| 2020-W17| 677.0|   Surveillance|https://www.sozia...|\n|Austria|Daily hospital oc...|2020-04-24| 2020-W17| 651.0|   Surveillance|https://www.sozia...|\n|Austria|Daily hospital oc...|2020-04-27| 2020-W18| 579.0|   Surveillance|https://www.sozia...|\n|Austria|Daily hospital oc...|2020-04-28| 2020-W18| 561.0|   Surveillance|https://info.gesu...|\n|Austria|Daily hospital oc...|2020-04-29| 2020-W18| 517.0|   Surveillance|https://www.sozia...|\n|Austria|Daily hospital oc...|2020-04-30| 2020-W18| 500.0|Country_Website|                NULL|\n|Austria|Daily hospital oc...|2020-05-02| 2020-W18| 428.0|Country_Website|                NULL|\n|Austria|Daily hospital oc...|2020-05-03| 2020-W18| 422.0|Country_Website|                NULL|\n|Austria|Daily hospital oc...|2020-05-04| 2020-W19| 420.0|Country_Website|                NULL|\n|Austria|Daily hospital oc...|2020-05-05| 2020-W19| 418.0|Country_Website|                NULL|\n|Austria|Daily hospital oc...|2020-05-06| 2020-W19| 418.0|Country_Website|                NULL|\n|Austria|Daily hospital oc...|2020-05-07| 2020-W19| 360.0|Country_Website|                NULL|\n+-------+--------------------+----------+---------+------+---------------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "hospital_admissions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6176388-7e17-4527-b672-6d36080bf0bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----------+---------+------+\n|country|           indicator|      date|year_week| value|\n+-------+--------------------+----------+---------+------+\n|Austria|Daily hospital oc...|2020-04-02| 2020-W14|1057.0|\n|Austria|Daily hospital oc...|2020-04-08| 2020-W15|1096.0|\n|Austria|Daily hospital oc...|2020-04-15| 2020-W16|1001.0|\n|Austria|Daily hospital oc...|2020-04-16| 2020-W16| 967.0|\n|Austria|Daily hospital oc...|2020-04-17| 2020-W16| 909.0|\n|Austria|Daily hospital oc...|2020-04-19| 2020-W16| 817.0|\n|Austria|Daily hospital oc...|2020-04-21| 2020-W17| 756.0|\n|Austria|Daily hospital oc...|2020-04-22| 2020-W17| 700.0|\n|Austria|Daily hospital oc...|2020-04-23| 2020-W17| 677.0|\n|Austria|Daily hospital oc...|2020-04-24| 2020-W17| 651.0|\n|Austria|Daily hospital oc...|2020-04-27| 2020-W18| 579.0|\n|Austria|Daily hospital oc...|2020-04-28| 2020-W18| 561.0|\n|Austria|Daily hospital oc...|2020-04-29| 2020-W18| 517.0|\n|Austria|Daily hospital oc...|2020-04-30| 2020-W18| 500.0|\n|Austria|Daily hospital oc...|2020-05-02| 2020-W18| 428.0|\n|Austria|Daily hospital oc...|2020-05-03| 2020-W18| 422.0|\n|Austria|Daily hospital oc...|2020-05-04| 2020-W19| 420.0|\n|Austria|Daily hospital oc...|2020-05-05| 2020-W19| 418.0|\n|Austria|Daily hospital oc...|2020-05-06| 2020-W19| 418.0|\n|Austria|Daily hospital oc...|2020-05-07| 2020-W19| 360.0|\n+-------+--------------------+----------+---------+------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "columns_needed = ['country','indicator','date','year_week','value']\n",
    "hospital_admissions_df = hospital_admissions.select(columns_needed)\n",
    "hospital_admissions_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e659048-15fa-4d54-ac22-4132f712ec67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----------+---------+------+\n|country|           indicator|      date|year_week| value|\n+-------+--------------------+----------+---------+------+\n|Austria|Daily hospital oc...|2020-04-02| 2020-W14|1057.0|\n|Austria|Daily hospital oc...|2020-04-08| 2020-W15|1096.0|\n|Austria|Daily hospital oc...|2020-04-15| 2020-W16|1001.0|\n|Austria|Daily hospital oc...|2020-04-16| 2020-W16| 967.0|\n|Austria|Daily hospital oc...|2020-04-17| 2020-W16| 909.0|\n|Austria|Daily hospital oc...|2020-04-19| 2020-W16| 817.0|\n|Austria|Daily hospital oc...|2020-04-21| 2020-W17| 756.0|\n|Austria|Daily hospital oc...|2020-04-22| 2020-W17| 700.0|\n|Austria|Daily hospital oc...|2020-04-23| 2020-W17| 677.0|\n|Austria|Daily hospital oc...|2020-04-24| 2020-W17| 651.0|\n|Austria|Daily hospital oc...|2020-04-27| 2020-W18| 579.0|\n|Austria|Daily hospital oc...|2020-04-28| 2020-W18| 561.0|\n|Austria|Daily hospital oc...|2020-04-29| 2020-W18| 517.0|\n|Austria|Daily hospital oc...|2020-04-30| 2020-W18| 500.0|\n|Austria|Daily hospital oc...|2020-05-02| 2020-W18| 428.0|\n|Austria|Daily hospital oc...|2020-05-03| 2020-W18| 422.0|\n|Austria|Daily hospital oc...|2020-05-04| 2020-W19| 420.0|\n|Austria|Daily hospital oc...|2020-05-05| 2020-W19| 418.0|\n|Austria|Daily hospital oc...|2020-05-06| 2020-W19| 418.0|\n|Austria|Daily hospital oc...|2020-05-07| 2020-W19| 360.0|\n+-------+--------------------+----------+---------+------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "hospital_admissions_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "657c6d78-d6da-4ec0-b4cd-60856e01eee3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Going to format the dates to make it easier to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebed2211-2f32-4066-a5f3-e3ac41acaba4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----------+---------+------+----+\n|country|           indicator|      date|year_week| value|week|\n+-------+--------------------+----------+---------+------+----+\n|Austria|Daily hospital oc...|2020-04-02| 2020-W14|1057.0|  14|\n|Austria|Daily hospital oc...|2020-04-08| 2020-W15|1096.0|  15|\n|Austria|Daily hospital oc...|2020-04-15| 2020-W16|1001.0|  16|\n|Austria|Daily hospital oc...|2020-04-16| 2020-W16| 967.0|  16|\n|Austria|Daily hospital oc...|2020-04-17| 2020-W16| 909.0|  16|\n|Austria|Daily hospital oc...|2020-04-19| 2020-W16| 817.0|  16|\n|Austria|Daily hospital oc...|2020-04-21| 2020-W17| 756.0|  17|\n|Austria|Daily hospital oc...|2020-04-22| 2020-W17| 700.0|  17|\n|Austria|Daily hospital oc...|2020-04-23| 2020-W17| 677.0|  17|\n|Austria|Daily hospital oc...|2020-04-24| 2020-W17| 651.0|  17|\n|Austria|Daily hospital oc...|2020-04-27| 2020-W18| 579.0|  18|\n|Austria|Daily hospital oc...|2020-04-28| 2020-W18| 561.0|  18|\n|Austria|Daily hospital oc...|2020-04-29| 2020-W18| 517.0|  18|\n|Austria|Daily hospital oc...|2020-04-30| 2020-W18| 500.0|  18|\n|Austria|Daily hospital oc...|2020-05-02| 2020-W18| 428.0|  18|\n|Austria|Daily hospital oc...|2020-05-03| 2020-W18| 422.0|  18|\n|Austria|Daily hospital oc...|2020-05-04| 2020-W19| 420.0|  19|\n|Austria|Daily hospital oc...|2020-05-05| 2020-W19| 418.0|  19|\n|Austria|Daily hospital oc...|2020-05-06| 2020-W19| 418.0|  19|\n|Austria|Daily hospital oc...|2020-05-07| 2020-W19| 360.0|  19|\n+-------+--------------------+----------+---------+------+----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create a new week column with the last 2 characters of the year_week column\n",
    "hospital_admissions_df = hospital_admissions_df.withColumn(\"week\", col(\"year_week\").substr(7, 2))\n",
    "\n",
    "\n",
    "hospital_admissions_df.show()\n",
    "\n",
    "# Get rid of year_week column\n",
    "hospital_admissions_df  = hospital_admissions_df.drop('year_week')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "814e475e-20d0-49e6-bbec-0511ba1bbd91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n|           indicator|\n+--------------------+\n|Daily hospital oc...|\n| Daily ICU occupancy|\n|Weekly new hospit...|\n|Weekly new ICU ad...|\n+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "unique_indicator = hospital_admissions_df.select('indicator').distinct()\n",
    "unique_indicator.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fa438be-cfbf-4176-93cf-d6b93f698a3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----------+-----+----+----+\n|country|           indicator|      date|value|year|week|\n+-------+--------------------+----------+-----+----+----+\n|Austria|Daily hospital oc...|2020-04-21|756.0|2020|  17|\n|Austria|Daily hospital oc...|2020-04-22|700.0|2020|  17|\n|Austria|Daily hospital oc...|2020-04-23|677.0|2020|  17|\n|Austria|Daily hospital oc...|2020-04-24|651.0|2020|  17|\n|Austria| Daily ICU occupancy|2020-04-21|196.0|2020|  17|\n|Austria| Daily ICU occupancy|2020-04-22|176.0|2020|  17|\n|Austria| Daily ICU occupancy|2020-04-23|169.0|2020|  17|\n|Austria| Daily ICU occupancy|2020-04-24|156.0|2020|  17|\n+-------+--------------------+----------+-----+----+----+\n\n"
     ]
    }
   ],
   "source": [
    "filtered_data = hospital_admissions_df.filter((hospital_admissions_df['country'] == 'Austria') & (hospital_admissions_df['week'] == '17'))\n",
    "filtered_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f5051a2-2dd0-428b-a71d-1e83533e2eef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "I think making a Daily table and Weekly table would make sense. So summing up the data for each week and having a weekly table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85c229db-df6b-4451-9ab1-7f5ec94e8886",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------+\n|week|week_start_date|\n+----+---------------+\n|  07|     2020-02-10|\n|  15|     2020-04-06|\n|  11|     2020-03-09|\n|  29|     2020-07-13|\n|  42|     2020-10-12|\n|  30|     2020-07-20|\n|  34|     2020-08-17|\n|  22|     2020-05-25|\n|  28|     2020-07-06|\n|  16|     2020-04-13|\n|  35|     2020-08-24|\n|  43|     2020-10-19|\n|  31|     2020-07-27|\n|  18|     2020-04-27|\n|  27|     2020-06-29|\n|  17|     2020-04-20|\n|  26|     2020-06-22|\n|  09|     2020-02-24|\n|  05|     2020-02-02|\n|  19|     2020-05-04|\n+----+---------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "# Get the week start date\n",
    "week_start_df = hospital_admissions_df.groupBy(\"week\").agg(\n",
    "    F.min(\"date\").alias(\"week_start_date\")  # Get the earliest date for each week\n",
    ")\n",
    "\n",
    "# Show the resulting DataFrame with week start dates\n",
    "week_start_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32046016-2bc5-4b77-990e-ea0d8372e98c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+--------------------+----------+------+---------------+\n|week|country|           indicator|      date| value|week_start_date|\n+----+-------+--------------------+----------+------+---------------+\n|  14|Austria|Daily hospital oc...|2020-04-02|1057.0|     2020-03-30|\n|  15|Austria|Daily hospital oc...|2020-04-08|1096.0|     2020-04-06|\n|  16|Austria|Daily hospital oc...|2020-04-15|1001.0|     2020-04-13|\n|  16|Austria|Daily hospital oc...|2020-04-16| 967.0|     2020-04-13|\n|  16|Austria|Daily hospital oc...|2020-04-17| 909.0|     2020-04-13|\n|  16|Austria|Daily hospital oc...|2020-04-19| 817.0|     2020-04-13|\n|  17|Austria|Daily hospital oc...|2020-04-21| 756.0|     2020-04-20|\n|  17|Austria|Daily hospital oc...|2020-04-22| 700.0|     2020-04-20|\n|  17|Austria|Daily hospital oc...|2020-04-23| 677.0|     2020-04-20|\n|  17|Austria|Daily hospital oc...|2020-04-24| 651.0|     2020-04-20|\n|  18|Austria|Daily hospital oc...|2020-04-27| 579.0|     2020-04-27|\n|  18|Austria|Daily hospital oc...|2020-04-28| 561.0|     2020-04-27|\n|  18|Austria|Daily hospital oc...|2020-04-29| 517.0|     2020-04-27|\n|  18|Austria|Daily hospital oc...|2020-04-30| 500.0|     2020-04-27|\n|  18|Austria|Daily hospital oc...|2020-05-02| 428.0|     2020-04-27|\n|  18|Austria|Daily hospital oc...|2020-05-03| 422.0|     2020-04-27|\n|  19|Austria|Daily hospital oc...|2020-05-04| 420.0|     2020-05-04|\n|  19|Austria|Daily hospital oc...|2020-05-05| 418.0|     2020-05-04|\n|  19|Austria|Daily hospital oc...|2020-05-06| 418.0|     2020-05-04|\n|  19|Austria|Daily hospital oc...|2020-05-07| 360.0|     2020-05-04|\n+----+-------+--------------------+----------+------+---------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Join to the main dataframe to add week start\n",
    "hospital_admissions_df = hospital_admissions_df.join(\n",
    "    week_start_df, on=\"week\", how=\"left\"  \n",
    ")\n",
    "\n",
    "hospital_admissions_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be964edd-c13b-48bc-80c6-1d9fc17e0ef4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+----+------------------+\n|           indicator|       country|week|        sum(value)|\n+--------------------+--------------+----+------------------+\n|Daily hospital oc...|       Denmark|  34|             133.0|\n| Daily ICU occupancy|       Denmark|  17|             528.0|\n|Weekly new hospit...|       Germany|  07|0.0409543752239617|\n| Daily ICU occupancy|       Ireland|  26|              82.0|\n| Daily ICU occupancy|   Netherlands|  41|            1790.0|\n|Weekly new hospit...|      Portugal|  32| 0.788197127517743|\n| Daily ICU occupancy|       Romania|  31|            2711.0|\n|Daily hospital oc...|      Slovenia|  32|             158.0|\n|Weekly new hospit...|      Slovenia|  17| 0.624727282513211|\n|Daily hospital oc...|United Kingdom|  35|            5537.0|\n| Daily ICU occupancy|       Austria|  19|             636.0|\n| Daily ICU occupancy|       Belgium|  25|             407.0|\n|Weekly new hospit...|        Cyprus|  24|  0.11416841439481|\n|Weekly new hospit...|       Czechia|  08|               0.0|\n|Weekly new hospit...|       Czechia|  14|  4.96722943153862|\n|Daily hospital oc...|       Finland|  37|              30.0|\n| Daily ICU occupancy|       Finland|  33|               2.0|\n|Weekly new ICU ad...|        France|  38| 0.893857976532662|\n|Weekly new ICU ad...|        Greece|  17|0.0932435795501538|\n| Daily ICU occupancy|       Iceland|  09|               0.0|\n+--------------------+--------------+----+------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Now make a new DataFrame for the weekly counts\n",
    "weekly_hospital_admissions = hospital_admissions_df.groupBy(['indicator','country','week','week_start_date']).sum()\n",
    "weekly_hospital_admissions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea404ad1-ad62-42db-aed4-7d5cea254e14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+----+----------+\n|           indicator|country|week|sum(value)|\n+--------------------+-------+----+----------+\n| Daily ICU occupancy|Austria|  17|     697.0|\n|Daily hospital oc...|Austria|  17|    2784.0|\n+--------------------+-------+----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Test with one country and week\n",
    "filtered_weekly = weekly_hospital_admissions.filter((weekly_hospital_admissions['country'] == 'Austria') & (weekly_hospital_admissions['week'] == '17'))\n",
    "filtered_weekly.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7465cd65-ec33-4b6b-ae04-3dd0d697ac92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "I want to pivot the data to make ICU occupany and hospital occupancy a column and get rid of indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28b03a67-b2fa-4ab2-bc5b-3e6a3f3fa8e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-------------------+------------------------+\n|country|week|Daily ICU occupancy|Daily hospital occupancy|\n+-------+----+-------------------+------------------------+\n|Austria|  17|              697.0|                  2784.0|\n+-------+----+-------------------+------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Pivot the DataFrame based on the 'indicator' column\n",
    "pivoted_df = filtered_weekly.groupBy(\"country\", \"week\") \\\n",
    "    .pivot(\"indicator\") \\\n",
    "    .agg({\"sum(value)\": \"first\"})\n",
    "\n",
    "pivoted_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddd2dcaf-727f-478b-bbc8-d14ea58dce55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+-------------------+------------------------+----------------------------------+---------------------------------------+\n|       country|week|Daily ICU occupancy|Daily hospital occupancy|Weekly new ICU admissions per 100k|Weekly new hospital admissions per 100k|\n+--------------+----+-------------------+------------------------+----------------------------------+---------------------------------------+\n|       Belgium|  19|             3732.0|                 18029.0|                              NULL|                       5.44715608258343|\n|United Kingdom|  24|               NULL|                 34026.0|                              NULL|                       4.50882252782386|\n|   Netherlands|  21|             1807.0|                    NULL|                 0.133085193097646|                       0.34139245185918|\n|        France|  18|            28344.0|                185788.0|                  1.08337377456212|                                   NULL|\n|       Ireland|  30|               42.0|                    84.0|                0.0611715576725446|                      0.224295711465997|\n|      Bulgaria|  20|              350.0|                  2449.0|                              NULL|                                   NULL|\n|      Slovenia|  13|              120.0|                   596.0|                              NULL|                       3.60419586065314|\n|         Spain|  36|               NULL|                    NULL|                 0.798942243080415|                        8.1300362655863|\n|       Ireland|  34|               45.0|                   135.0|                 0.040781038448363|                      0.346638826811086|\n|        Norway|  22|               NULL|                   235.0|                              NULL|                                   NULL|\n|       Germany|  25|               NULL|                    NULL|                              NULL|                      0.289089707463259|\n|       Belgium|  43|             4113.0|                 25956.0|                              NULL|                       28.3793340135877|\n|      Slovakia|  31|               NULL|                   182.0|                              NULL|                                   NULL|\n|        Norway|  15|               NULL|                  1755.0|                              NULL|                                   NULL|\n|        Sweden|  18|               NULL|                    NULL|                  1.83769892724325|                                   NULL|\n|       Belgium|  07|               NULL|                    NULL|                              NULL|                                   NULL|\n|       Estonia|  35|                0.0|                    70.0|                               0.0|                      0.150963904530427|\n|       Belgium|  37|              442.0|                  1801.0|                              NULL|                       1.85936577818954|\n|       Belgium|  34|              601.0|                  2249.0|                              NULL|                       1.48400085583202|\n|   Netherlands|  24|              761.0|                    NULL|                0.0578631274337593|                      0.138871505841022|\n+--------------+----+-------------------+------------------------+----------------------------------+---------------------------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "weekly_hospital_admissions = weekly_hospital_admissions.groupBy(\"country\", \"week\").pivot(\"indicator\").agg({\"sum(value)\": \"first\"})\n",
    "\n",
    "weekly_hospital_admissions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6be34227-28d6-4487-bcf2-c6bb758b639a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+-------------------+------------------------+\n|       country|week|Daily ICU occupancy|Daily hospital occupancy|\n+--------------+----+-------------------+------------------------+\n|       Belgium|  19|             3732.0|                 18029.0|\n|United Kingdom|  24|               NULL|                 34026.0|\n|   Netherlands|  21|             1807.0|                    NULL|\n|        France|  18|            28344.0|                185788.0|\n|       Ireland|  30|               42.0|                    84.0|\n|      Bulgaria|  20|              350.0|                  2449.0|\n|      Slovenia|  13|              120.0|                   596.0|\n|         Spain|  36|               NULL|                    NULL|\n|       Ireland|  34|               45.0|                   135.0|\n|        Norway|  22|               NULL|                   235.0|\n|       Germany|  25|               NULL|                    NULL|\n|       Belgium|  43|             4113.0|                 25956.0|\n|      Slovakia|  31|               NULL|                   182.0|\n|        Norway|  15|               NULL|                  1755.0|\n|        Sweden|  18|               NULL|                    NULL|\n|       Belgium|  07|               NULL|                    NULL|\n|       Estonia|  35|                0.0|                    70.0|\n|       Belgium|  37|              442.0|                  1801.0|\n|       Belgium|  34|              601.0|                  2249.0|\n|   Netherlands|  24|              761.0|                    NULL|\n+--------------+----+-------------------+------------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Only grab columns we need\n",
    "weekly_hospital_admissions = weekly_hospital_admissions.select(\"country\", \"week\", \"Daily ICU occupancy\",\"Daily hospital occupancy\")\n",
    "\n",
    "#Rename columns as well\n",
    "weekly_hospital_admissions = weekly_hospital_admissions.withColumnRenamed(\"Daily ICU occupancy`\", \"weekly_ICU_occupancy\")\n",
    "weekly_hospital_admissions = weekly_hospital_admissions.withColumnRenamed(\"Daily hospital occupancy`\", \"weekly_hospital_occupancy\")\n",
    "\n",
    "weekly_hospital_admissions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be45b8d0-3475-434d-a493-d158b6f49cbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----+-------------------+------------------------+----------------------------------+---------------------------------------+\n|    country|      date|week|Daily ICU occupancy|Daily hospital occupancy|Weekly new ICU admissions per 100k|Weekly new hospital admissions per 100k|\n+-----------+----------+----+-------------------+------------------------+----------------------------------+---------------------------------------+\n| Luxembourg|2020-10-05|  41|                1.0|                    22.0|                              NULL|                                   NULL|\n|     France|2020-05-29|  22|             1328.0|                 14643.0|                              NULL|                                   NULL|\n|    Czechia|2020-03-17|  12|                4.0|                    50.0|                              NULL|                                   NULL|\n|    Austria|2020-07-20|  30|               16.0|                   112.0|                              NULL|                                   NULL|\n|     France|2020-03-26|  13|             3351.0|                 13879.0|                              NULL|                                   NULL|\n|Netherlands|      NULL|  21|               NULL|                    NULL|                 0.133085193097646|                       0.34139245185918|\n|    Finland|2020-06-22|  26|                2.0|                    21.0|                              NULL|                                   NULL|\n|    Finland|2020-06-13|  24|                2.0|                    26.0|                              NULL|                                   NULL|\n|    Iceland|2020-06-14|  24|                0.0|                     0.0|                              NULL|                                   NULL|\n|     Latvia|2020-07-20|  30|               NULL|                     3.0|                              NULL|                                   NULL|\n|    Estonia|2020-09-14|  38|                1.0|                    19.0|                              NULL|                                   NULL|\n|    Czechia|2020-09-01|  36|               40.0|                   172.0|                              NULL|                                   NULL|\n|    Croatia|2020-10-16|  42|               NULL|                   505.0|                              NULL|                                   NULL|\n|    Ireland|      NULL|  30|               NULL|                    NULL|                0.0611715576725446|                      0.224295711465997|\n|    Denmark|2020-06-02|  23|               21.0|                    99.0|                              NULL|                                   NULL|\n|    Hungary|2020-07-10|  28|                6.0|                   133.0|                              NULL|                                   NULL|\n| Luxembourg|2020-09-02|  36|                2.0|                    16.0|                              NULL|                                   NULL|\n|   Slovenia|2020-10-24|  43|               66.0|                   450.0|                              NULL|                                   NULL|\n|   Portugal|2020-06-24|  26|               73.0|                   429.0|                              NULL|                                   NULL|\n|    Belgium|2020-08-30|  35|               71.0|                   233.0|                              NULL|                                   NULL|\n+-----------+----------+----+-------------------+------------------------+----------------------------------+---------------------------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#Now also going to pivot the daily table\n",
    "hospital_admissions_df = hospital_admissions_df.groupBy(\"country\", \"date\", \"week\").pivot(\"indicator\").agg({\"value\": \"first\"})\n",
    "\n",
    "hospital_admissions_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52426134-b502-41a6-b26f-d58a3f5af652",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----+-------------------+------------------------+\n|    country|      date|week|Daily ICU occupancy|Daily hospital occupancy|\n+-----------+----------+----+-------------------+------------------------+\n| Luxembourg|2020-10-05|  41|                1.0|                    22.0|\n|     France|2020-05-29|  22|             1328.0|                 14643.0|\n|    Czechia|2020-03-17|  12|                4.0|                    50.0|\n|    Austria|2020-07-20|  30|               16.0|                   112.0|\n|     France|2020-03-26|  13|             3351.0|                 13879.0|\n|Netherlands|      NULL|  21|               NULL|                    NULL|\n|    Finland|2020-06-22|  26|                2.0|                    21.0|\n|    Finland|2020-06-13|  24|                2.0|                    26.0|\n|    Iceland|2020-06-14|  24|                0.0|                     0.0|\n|     Latvia|2020-07-20|  30|               NULL|                     3.0|\n|    Estonia|2020-09-14|  38|                1.0|                    19.0|\n|    Czechia|2020-09-01|  36|               40.0|                   172.0|\n|    Croatia|2020-10-16|  42|               NULL|                   505.0|\n|    Ireland|      NULL|  30|               NULL|                    NULL|\n|    Denmark|2020-06-02|  23|               21.0|                    99.0|\n|    Hungary|2020-07-10|  28|                6.0|                   133.0|\n| Luxembourg|2020-09-02|  36|                2.0|                    16.0|\n|   Slovenia|2020-10-24|  43|               66.0|                   450.0|\n|   Portugal|2020-06-24|  26|               73.0|                   429.0|\n|    Belgium|2020-08-30|  35|               71.0|                   233.0|\n+-----------+----------+----+-------------------+------------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#Drop last 2 columns\n",
    "# Get the names of the last two columns\n",
    "columns_to_drop = hospital_admissions_df.columns[-2:]\n",
    "\n",
    "# Create a new DataFrame without the last two columns\n",
    "hospital_admissions_df = hospital_admissions_df.drop(*columns_to_drop)\n",
    "\n",
    "hospital_admissions_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01e32b0e-5222-4e18-9d82-fd4ed5cc0aea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-413928462589528>, line 5\u001B[0m\n",
       "\u001B[1;32m      2\u001B[0m output_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mabfss://silver@adlsmzubac125.dfs.core.windows.net/ecdc_data/dailyhospitaladmissions.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Example: Saving a DataFrame as a CSV to Azure Data Lake\u001B[39;00m\n",
       "\u001B[0;32m----> 5\u001B[0m hospital_admissions_df\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mcsv(output_path)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     50\u001B[0m     )\n",
       "\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:2133\u001B[0m, in \u001B[0;36mDataFrameWriter.csv\u001B[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001B[0m\n",
       "\u001B[1;32m   2114\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode(mode)\n",
       "\u001B[1;32m   2115\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_opts(\n",
       "\u001B[1;32m   2116\u001B[0m     compression\u001B[38;5;241m=\u001B[39mcompression,\n",
       "\u001B[1;32m   2117\u001B[0m     sep\u001B[38;5;241m=\u001B[39msep,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   2131\u001B[0m     lineSep\u001B[38;5;241m=\u001B[39mlineSep,\n",
       "\u001B[1;32m   2132\u001B[0m )\n",
       "\u001B[0;32m-> 2133\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39mcsv(path)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n",
       "\u001B[1;32m   1356\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n",
       "\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:255\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    252\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpy4j\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprotocol\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Py4JJavaError\n",
       "\u001B[1;32m    254\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 255\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39ma, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw)\n",
       "\u001B[1;32m    256\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    257\u001B[0m     converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n",
       "\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n",
       "\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n",
       "\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n",
       "\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n",
       "\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n",
       "\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
       "\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o443.csv.\n",
       ": Failure to initialize configuration for storage account adlsmzubac125.dfs.core.windows.net: Invalid configuration value detected for fs.azure.account.keyInvalid configuration value detected for fs.azure.account.key\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.getStorageAccountKey(SimpleKeyProvider.java:52)\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AbfsConfiguration.getStorageAccountKey(AbfsConfiguration.java:715)\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.initializeClient(AzureBlobFileSystemStore.java:2100)\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.<init>(AzureBlobFileSystemStore.java:272)\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.initialize(AzureBlobFileSystem.java:239)\n",
       "\tat com.databricks.common.filesystem.LokiABFS.initialize(LokiABFS.scala:36)\n",
       "\tat com.databricks.common.filesystem.LokiFileSystem$.$anonfun$getLokiFS$1(LokiFileSystem.scala:163)\n",
       "\tat com.databricks.common.filesystem.FileSystemCache.getOrCompute(FileSystemCache.scala:43)\n",
       "\tat com.databricks.common.filesystem.LokiFileSystem$.getLokiFS(LokiFileSystem.scala:159)\n",
       "\tat com.databricks.common.filesystem.LokiFileSystem.initialize(LokiFileSystem.scala:253)\n",
       "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3611)\n",
       "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:554)\n",
       "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
       "\tat com.databricks.unity.SAM.createDelegate(SAM.scala:327)\n",
       "\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:315)\n",
       "\tat com.databricks.unity.FallbackToClusterDefaultSAM.createDelegate(SAM.scala:386)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:84)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:137)\n",
       "\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n",
       "\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.getWorkingDirectory(CredentialScopeFileSystem.scala:260)\n",
       "\tat org.apache.hadoop.fs.FileSystem.makeQualified(FileSystem.java:684)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:262)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaValidation$.validateNonDeltaWrite(DeltaValidation.scala:200)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:297)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:264)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:1000)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: Invalid configuration value detected for fs.azure.account.key\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.diagnostics.ConfigurationBasicValidator.validate(ConfigurationBasicValidator.java:49)\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.diagnostics.Base64StringConfigurationBasicValidator.validate(Base64StringConfigurationBasicValidator.java:40)\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.validateStorageAccountKey(SimpleKeyProvider.java:71)\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.getStorageAccountKey(SimpleKeyProvider.java:49)\n",
       "\t... 37 more\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "Py4JJavaError",
        "evalue": "An error occurred while calling o443.csv.\n: Failure to initialize configuration for storage account adlsmzubac125.dfs.core.windows.net: Invalid configuration value detected for fs.azure.account.keyInvalid configuration value detected for fs.azure.account.key\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.getStorageAccountKey(SimpleKeyProvider.java:52)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AbfsConfiguration.getStorageAccountKey(AbfsConfiguration.java:715)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.initializeClient(AzureBlobFileSystemStore.java:2100)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.<init>(AzureBlobFileSystemStore.java:272)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.initialize(AzureBlobFileSystem.java:239)\n\tat com.databricks.common.filesystem.LokiABFS.initialize(LokiABFS.scala:36)\n\tat com.databricks.common.filesystem.LokiFileSystem$.$anonfun$getLokiFS$1(LokiFileSystem.scala:163)\n\tat com.databricks.common.filesystem.FileSystemCache.getOrCompute(FileSystemCache.scala:43)\n\tat com.databricks.common.filesystem.LokiFileSystem$.getLokiFS(LokiFileSystem.scala:159)\n\tat com.databricks.common.filesystem.LokiFileSystem.initialize(LokiFileSystem.scala:253)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3611)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:554)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat com.databricks.unity.SAM.createDelegate(SAM.scala:327)\n\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:315)\n\tat com.databricks.unity.FallbackToClusterDefaultSAM.createDelegate(SAM.scala:386)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:84)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:137)\n\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.getWorkingDirectory(CredentialScopeFileSystem.scala:260)\n\tat org.apache.hadoop.fs.FileSystem.makeQualified(FileSystem.java:684)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:262)\n\tat com.databricks.sql.transaction.tahoe.DeltaValidation$.validateNonDeltaWrite(DeltaValidation.scala:200)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:297)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:264)\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:1000)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: Invalid configuration value detected for fs.azure.account.key\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.diagnostics.ConfigurationBasicValidator.validate(ConfigurationBasicValidator.java:49)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.diagnostics.Base64StringConfigurationBasicValidator.validate(Base64StringConfigurationBasicValidator.java:40)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.validateStorageAccountKey(SimpleKeyProvider.java:71)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.getStorageAccountKey(SimpleKeyProvider.java:49)\n\t... 37 more\n"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>Py4JJavaError</span>: An error occurred while calling o443.csv.\n: Failure to initialize configuration for storage account adlsmzubac125.dfs.core.windows.net: Invalid configuration value detected for fs.azure.account.keyInvalid configuration value detected for fs.azure.account.key\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.getStorageAccountKey(SimpleKeyProvider.java:52)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AbfsConfiguration.getStorageAccountKey(AbfsConfiguration.java:715)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.initializeClient(AzureBlobFileSystemStore.java:2100)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.<init>(AzureBlobFileSystemStore.java:272)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.initialize(AzureBlobFileSystem.java:239)\n\tat com.databricks.common.filesystem.LokiABFS.initialize(LokiABFS.scala:36)\n\tat com.databricks.common.filesystem.LokiFileSystem$.$anonfun$getLokiFS$1(LokiFileSystem.scala:163)\n\tat com.databricks.common.filesystem.FileSystemCache.getOrCompute(FileSystemCache.scala:43)\n\tat com.databricks.common.filesystem.LokiFileSystem$.getLokiFS(LokiFileSystem.scala:159)\n\tat com.databricks.common.filesystem.LokiFileSystem.initialize(LokiFileSystem.scala:253)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3611)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:554)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat com.databricks.unity.SAM.createDelegate(SAM.scala:327)\n\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:315)\n\tat com.databricks.unity.FallbackToClusterDefaultSAM.createDelegate(SAM.scala:386)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:84)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:137)\n\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.getWorkingDirectory(CredentialScopeFileSystem.scala:260)\n\tat org.apache.hadoop.fs.FileSystem.makeQualified(FileSystem.java:684)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:262)\n\tat com.databricks.sql.transaction.tahoe.DeltaValidation$.validateNonDeltaWrite(DeltaValidation.scala:200)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:297)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:264)\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:1000)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: Invalid configuration value detected for fs.azure.account.key\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.diagnostics.ConfigurationBasicValidator.validate(ConfigurationBasicValidator.java:49)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.diagnostics.Base64StringConfigurationBasicValidator.validate(Base64StringConfigurationBasicValidator.java:40)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.validateStorageAccountKey(SimpleKeyProvider.java:71)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.getStorageAccountKey(SimpleKeyProvider.java:49)\n\t... 37 more\n"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
        "File \u001B[0;32m<command-413928462589528>, line 5\u001B[0m\n\u001B[1;32m      2\u001B[0m output_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mabfss://silver@adlsmzubac125.dfs.core.windows.net/ecdc_data/dailyhospitaladmissions.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Example: Saving a DataFrame as a CSV to Azure Data Lake\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m hospital_admissions_df\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mcsv(output_path)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     50\u001B[0m     )\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:2133\u001B[0m, in \u001B[0;36mDataFrameWriter.csv\u001B[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001B[0m\n\u001B[1;32m   2114\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode(mode)\n\u001B[1;32m   2115\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_opts(\n\u001B[1;32m   2116\u001B[0m     compression\u001B[38;5;241m=\u001B[39mcompression,\n\u001B[1;32m   2117\u001B[0m     sep\u001B[38;5;241m=\u001B[39msep,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2131\u001B[0m     lineSep\u001B[38;5;241m=\u001B[39mlineSep,\n\u001B[1;32m   2132\u001B[0m )\n\u001B[0;32m-> 2133\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39mcsv(path)\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1356\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:255\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    252\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpy4j\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprotocol\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Py4JJavaError\n\u001B[1;32m    254\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 255\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39ma, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw)\n\u001B[1;32m    256\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    257\u001B[0m     converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
        "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o443.csv.\n: Failure to initialize configuration for storage account adlsmzubac125.dfs.core.windows.net: Invalid configuration value detected for fs.azure.account.keyInvalid configuration value detected for fs.azure.account.key\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.getStorageAccountKey(SimpleKeyProvider.java:52)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AbfsConfiguration.getStorageAccountKey(AbfsConfiguration.java:715)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.initializeClient(AzureBlobFileSystemStore.java:2100)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.<init>(AzureBlobFileSystemStore.java:272)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.initialize(AzureBlobFileSystem.java:239)\n\tat com.databricks.common.filesystem.LokiABFS.initialize(LokiABFS.scala:36)\n\tat com.databricks.common.filesystem.LokiFileSystem$.$anonfun$getLokiFS$1(LokiFileSystem.scala:163)\n\tat com.databricks.common.filesystem.FileSystemCache.getOrCompute(FileSystemCache.scala:43)\n\tat com.databricks.common.filesystem.LokiFileSystem$.getLokiFS(LokiFileSystem.scala:159)\n\tat com.databricks.common.filesystem.LokiFileSystem.initialize(LokiFileSystem.scala:253)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3611)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:554)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat com.databricks.unity.SAM.createDelegate(SAM.scala:327)\n\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:315)\n\tat com.databricks.unity.FallbackToClusterDefaultSAM.createDelegate(SAM.scala:386)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:84)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:137)\n\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.getWorkingDirectory(CredentialScopeFileSystem.scala:260)\n\tat org.apache.hadoop.fs.FileSystem.makeQualified(FileSystem.java:684)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:262)\n\tat com.databricks.sql.transaction.tahoe.DeltaValidation$.validateNonDeltaWrite(DeltaValidation.scala:200)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:297)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:264)\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:1000)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: Invalid configuration value detected for fs.azure.account.key\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.diagnostics.ConfigurationBasicValidator.validate(ConfigurationBasicValidator.java:49)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.diagnostics.Base64StringConfigurationBasicValidator.validate(Base64StringConfigurationBasicValidator.java:40)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.validateStorageAccountKey(SimpleKeyProvider.java:71)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.getStorageAccountKey(SimpleKeyProvider.java:49)\n\t... 37 more\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save the transformed data into our silver container\n",
    "output_path = \"abfss://ecdc@adlsmzubac125.dfs.core.windows.net/silver/dailyhospitaladmissions.csv\"\n",
    "\n",
    "# Example: Saving a DataFrame as a CSV to Azure Data Lake\n",
    "hospital_admissions_df.write.csv(output_path)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 413928462589529,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Hospital Admissions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
